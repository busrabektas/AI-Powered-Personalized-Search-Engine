import json
import unicodedata
import re
from datasets import load_dataset
from sentence_transformers import SentenceTransformer


# ğŸ“Œ Modeli yÃ¼kle
model = SentenceTransformer("sentence-transformers/all-MiniLM-L6-v2")

# Ã–rnek veri yÃ¼kleme (Ä°lk 5 makaleyi almak iÃ§in)
dataset = load_dataset("wikimedia/wikipedia", "20231101.en", split="train[:5]")

def clean_unicode(text):
    """Unicode karakterlerini normalleÅŸtir ve gereksizleri temizle."""
    text = unicodedata.normalize("NFKC", text)  # Unicode normalizasyonu
    text = re.sub(r"\s+", " ", text)  # Gereksiz boÅŸluklarÄ± temizle
    text = re.sub(r"[^\w\s.,!?()\"'-]", "", text)  # Ã–zel sembolleri temizle
    return text.strip()

# ğŸ“Œ Dataset'i temizleyip embedding ekleyerek liste formatÄ±na Ã§evir
cleaned_articles = []
for article in dataset:
    cleaned_text = clean_unicode(article["text"])  # Unicode temizliÄŸi uygulanmÄ±ÅŸ metin
    embedding = model.encode(cleaned_text).tolist()  # Metni embedding'e Ã§evir

    cleaned_articles.append({
        "id": article["id"],
        "title": article["title"],
        "url": article["url"],
        "text": cleaned_text,
        "embedding": embedding  # Embedding ekledik
    })

# JSON dosyasÄ±na kaydet
file_path = "cleaned_wikipedia_embeddings.json"
with open(file_path, "w", encoding="utf-8") as f:
    json.dump(cleaned_articles, f, ensure_ascii=False, indent=4)

print(f"Unicode karakterleri dÃ¼zeltildi, embedding'ler eklendi ve '{file_path}' dosyasÄ±na kaydedildi.")
